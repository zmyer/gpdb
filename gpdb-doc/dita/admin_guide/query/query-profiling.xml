<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">

<topic id="topic39" xml:lang="en">
  <title id="in198649">Query Profiling</title>

  <body>
    <p>Greenplum Database devises a <i>query plan</i> for each query. Choosing the right query plan
      to match the query and data structure is necessary for good performance. A query plan defines
      how Greenplum Database will run the query in the parallel execution environment. Examine the
      query plans of poorly performing queries to identify possible performance tuning
      opportunities.</p>

    <p>The query planner uses data statistics maintained by the database to choose a query plan with
      the lowest possible cost. Cost is measured in disk I/O, shown as units of disk page fetches.
      The goal is to minimize the total execution cost for the plan.</p>

    <p>View the plan for a given query with the <codeph>EXPLAIN</codeph> command.
        <codeph>EXPLAIN</codeph> shows the query planner's estimated cost for the query plan. For
      example:</p>

    <p>
      <codeblock>EXPLAIN SELECT * FROM names WHERE id=22;
</codeblock>
    </p>

    <p><codeph>EXPLAIN ANALYZE</codeph> runs the statement in addition to displaying its plan. This
      is useful for determining how close the planner's estimates are to reality. For example:</p>

    <p>
      <codeblock>EXPLAIN ANALYZE SELECT * FROM names WHERE id=22;
</codeblock>
    </p>
  </body>

  <topic id="topic40" xml:lang="en">
    <title>Reading EXPLAIN Output</title>

    <body>
      <p>A query plan is a tree of nodes. Each node in the plan represents a single operation, such
        as a table scan, join, aggregation, or sort.</p>

      <p>Read plans from the bottom to the top: each node feeds rows into the node directly above
        it. The bottom nodes of a plan are usually table scan operations: sequential, index, or
        bitmap index scans. If the query requires joins, aggregations, sorts, or other operations on
        the rows, there are additional nodes above the scan nodes to perform these operations. The
        topmost plan nodes are usually Greenplum Database motion nodes: redistribute, explicit
        redistribute, broadcast, or gather motions. These operations move rows between segment
        instances during query processing.</p>

      <p>The output of <codeph>EXPLAIN</codeph> has one line for each node in the plan tree and
        shows the basic node type and the following execution cost estimates for that plan node:</p>

      <ul>
        <li id="in182482"><b>cost</b> —Measured in units of disk page fetches. 1.0 equals one
          sequential disk page read. The first estimate is the start-up cost of getting the first
          row and the second is the total cost of cost of getting all rows. The total cost assumes
          all rows will be retrieved, which is not always true; for example, if the query uses
            <codeph>LIMIT</codeph>, not all rows are retrieved.</li>

        <li id="in182483"><b>rows</b> —The total number of rows output by this plan node. This
          number is usually less than the number of rows processed or scanned by the plan node,
          reflecting the estimated selectivity of any <codeph>WHERE</codeph> clause conditions.
          Ideally, the estimate for the topmost node approximates the number of rows that the query
          actually returns, updates, or deletes.</li>

        <li id="in182484"><b>width</b> —The total bytes of all the rows that this plan node
          outputs.</li>
      </ul>

      <p>Note the following:</p>

      <ul>
        <li id="in203106">The cost of a node includes the cost of its child nodes. The topmost plan
          node has the estimated total execution cost for the plan. This is the number the planner
          intends to minimize.</li>

        <li id="in203119">The cost reflects only the aspects of plan execution that the query
          planner takes into consideration. For example, the cost does not reflect time spent
          transmitting result rows to the client.</li>
      </ul>
    </body>

    <topic id="topic41" xml:lang="en">
      <title id="in182487">EXPLAIN Example</title>

      <body>
        <p>The following example describes how to read an <codeph>EXPLAIN</codeph> query plan for a
          query:</p>

        <p>
          <codeblock>EXPLAIN SELECT * FROM names WHERE name = 'Joelle';
                     QUERY PLAN
------------------------------------------------------------
Gather Motion 2:1 (slice1) (cost=0.00..20.88 rows=1 width=13)

   -&gt; Seq Scan on 'names' (cost=0.00..20.88 rows=1 width=13)
         Filter: name::text ~~ 'Joelle'::text
</codeblock>
        </p>

        <p>Read the plan from the bottom to the top. To start, the query planner sequentially scans
          the <i>names</i> table. Notice the <codeph>WHERE</codeph> clause is applied as a
            <i>filter</i> condition. This means the scan operation checks the condition for each row
          it scans and outputs only the rows that satisfy the condition.</p>

        <p>The results of the scan operation are passed to a <i>gather motion</i> operation. In
          Greenplum Database, a gather motion is when segments send rows to the master. In this
          example, we have two segment instances that send to one master instance. This operation is
          working on <codeph>slice1</codeph> of the parallel query execution plan. A query plan is
          divided into <i>slices</i> so the segments can work on portions of the query plan in
          parallel.</p>

        <p>The estimated startup cost for this plan is <codeph>00.00</codeph> (no cost) and a total
          cost of <codeph>20.88</codeph> disk page fetches. The planner estimates this query will
          return one row.</p>
      </body>
    </topic>
  </topic>

  <topic id="topic42" xml:lang="en">
    <title>Reading EXPLAIN ANALYZE Output</title>

    <body>
      <p><codeph>EXPLAIN ANALYZE</codeph> plans and runs the statement. The <codeph>EXPLAIN
          ANALYZE</codeph> plan shows the actual execution cost along with the planner's estimates.
        This allows you to see if the planner's estimates are close to reality. <codeph>EXPLAIN
          ANALYZE</codeph> also shows the following:</p>

      <ul>
        <li id="in209045">The total runtime (in milliseconds) in which the query executed.</li>

        <li id="in209046">The memory used by each slice of the query plan, as well as the memory
          reserved for the whole query statement.</li>

        <li id="in209047">The number of <i>workers</i> (segments) involved in a plan node operation.
          Only segments that return rows are counted.</li>

        <li id="in209048">The maximum number of rows returned by the segment that produced the most
          rows for the operation. If multiple segments produce an equal number of rows,
            <codeph>EXPLAIN ANALYZE</codeph> shows the segment with the longest <i>&lt;time&gt; to
            end</i>.</li>

        <li id="in209049">The segment id of the segment that produced the most rows for an
          operation.</li>

        <li id="in209050">For relevant operations, the amount of memory (<codeph>work_mem</codeph>)
          used by the operation. If the <codeph>work_mem</codeph> was insufficient to perform the
          operation in memory, the plan shows the amount of data spilled to disk for the
          lowest-performing segment. For
            example:<p><codeblock>Work_mem used: 64K bytes avg, 64K bytes max (seg0).
Work_mem wanted: 90K bytes avg, 90K byes max (seg0) to lessen 
workfile I/O affecting 2 workers.
</codeblock></p></li>

        <li id="in209053">The time (in milliseconds) in which the segment that produced the most
          rows retrieved the first row, and the time taken for that segment to retrieve all rows.
          The result may omit <i>&lt;time&gt; to first row</i> if it is the same as the
            <i>&lt;time&gt; to end</i>.</li>
      </ul>
    </body>

    <topic id="topic43" xml:lang="en">
      <title>EXPLAIN ANALYZE Example</title>

      <body>
        <p>This example describes how to read an <codeph>EXPLAIN</codeph><codeph>ANALYZE</codeph>
          query plan using the same query. The <codeph>bold</codeph> parts of the plan show actual
          timing and rows returned for each plan node, as well as memory and time statistics for the
          whole query.</p>

        <codeblock>EXPLAIN ANALYZE SELECT * FROM names WHERE name = 'Joelle';
                     QUERY PLAN
------------------------------------------------------------
Gather Motion 2:1 (slice1; segments: 2) (cost=0.00..20.88 rows=1 width=13)
    Rows out: 1 rows at destination with 0.305 ms to first row, 0.537 ms to end, start offset by 0.289 ms.
        -&gt; Seq Scan on names (cost=0.00..20.88 rows=1 width=13)
             Rows out: Avg 1 rows x 2 workers. Max 1 rows (seg0) with 0.255 ms to first row, 0.486 ms to end, start offset by 0.968 ms.
                 Filter: name = 'Joelle'::text
 Slice statistics:

      (slice0) Executor memory: 135K bytes.

    (slice1) Executor memory: 151K bytes avg x 2 workers, 151K bytes max (seg0).

Statement statistics:
 Memory used: 128000K bytes
 Total runtime: 22.548 ms
</codeblock>

        <p>Read the plan from the bottom to the top. The total elapsed time to run this query was
            <i>22.548</i> milliseconds.</p>

        <p>The <i>sequential scan</i> operation had only one segment (<i>seg0</i>) that returned
          rows, and it returned just <i>1 row</i>. It took <i>0.255</i> milliseconds to find the
          first row and <i>0.486</i> to scan all rows. This result is close to the planner's
          estimate: the query planner estimated it would return one row for this query. The
            <i>gather motion</i> (segments sending data to the master) received 1 row . The total
          elapsed time for this operation was <i>0.537</i> milliseconds.</p>
      </body>
    </topic>
  </topic>

  <topic id="topic44" xml:lang="en">
    <title>Examining Query Plans to Solve Problems</title>

    <body>
      <p>If a query performs poorly, examine its query plan and ask the following questions:</p>

      <ul>
        <li id="in182530"><b>Do operations in the plan take an exceptionally long time?</b> Look for
          an operation consumes the majority of query processing time. For example, if an index scan
          takes longer than expected, the index could be out-of-date and need to be reindexed. Or,
          adjust <codeph>enable_&lt;operator&gt; </codeph>parameters to see if you can force the
          planner to choose a different plan by disabling a particular query plan operator for that
          query.</li>

        <li id="in182538"><b>Are the planner's estimates close to reality?</b> Run <codeph>EXPLAIN
            ANALYZE</codeph> and see if the number of rows the planner estimates is close to the
          number of rows the query operation actually returns. If there is a large discrepancy,
          collect more statistics on the relevant columns. See the <i>Greenplum Database Reference
            Guide</i> for more information on the <codeph>EXPLAIN ANALYZE</codeph> and
            <codeph>ANALYZE</codeph> commands.</li>

        <li id="in182542"><b>Are selective predicates applied early in the plan?</b> Apply the most
          selective filters early in the plan so fewer rows move up the plan tree. If the query plan
          does not correctly estimate query predicate selectivity, collect more statistics on the
          relevant columns. See the <codeph>ANALYZE</codeph> command in the <i>Greenplum Database
            Reference Guide</i> for more information collecting statistics. You can also try
          reordering the <codeph>WHERE</codeph> clause of your SQL statement.</li>

        <li id="in182546"><b>Does the planner choose the best join order?</b> When you have a query
          that joins multiple tables, make sure that the planner chooses the most selective join
          order. Joins that eliminate the largest number of rows should be done earlier in the plan
          so fewer rows move up the plan tree. <p>If the plan is not choosing the optimal join
            order, set <codeph>join_collapse_limit=1</codeph> and use explicit <codeph>JOIN</codeph>
            syntax in your SQL statement to force the planner to the specified join order. You can
            also collect more statistics on the relevant join columns. See the
              <codeph>ANALYZE</codeph> command in the <i>Greenplum Database Reference Guide</i> for
            more information collecting statistics.</p></li>

        <li id="in182550"><b>Does the planner selectively scan partitioned tables?</b> If you use
          table partitioning, is the planner selectively scanning only the child tables required to
          satisfy the query predicates? Scans of the parent tables should return 0 rows since the
          parent tables do not contain any data. See <xref href="../ddl/ddl-partition.xml#topic74"/>
          for an example of a query plan that shows a selective partition scan.</li>

        <li id="in182554"><b>Does the planner choose hash aggregate and hash join operations where
            applicable?</b> Hash operations are typically much faster than other types of joins or
          aggregations. Row comparison and sorting is done in memory rather than reading/writing
          from disk. To enable the query planner to choose hash operations, there must be sufficient
          memory available to hold the estimated number of rows. Try increasing work memory to
          improve performance for a query. If possible, run an <codeph>EXPLAIN ANALYZE</codeph> for
          the query to show which plan operations spilled to disk, how much work memory they used,
          and how much memory was required to avoid spilling to disk. For
              example:<p><codeph>Work_mem used: 23430K bytes avg, 23430K bytes max (seg0). Work_mem
              wanted: 33649K bytes avg, 33649K bytes max (seg0) to lessen workfile I/O affecting 2
              workers.</codeph></p><p>The "bytes wanted" message from <codeph>EXPLAIN
              ANALYZE</codeph> is based on the amount of data written to work files and is not
            exact. The minimum <codeph>work_mem</codeph> needed can differ from the suggested
            value.</p></li>
      </ul>
    </body>
  </topic>
</topic>
