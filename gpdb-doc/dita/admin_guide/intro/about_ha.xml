<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Composite//EN"
"ditabase.dtd">

<topic id="about_ha" xml:lang="en">

  <title id="iw157531">About Redundancy and Failover in Greenplum Database</title>
  <shortdesc>This topic provides a high-level overview of Greenplum Database high availability
    features.</shortdesc>
  <body>
    <p>You can deploy Greenplum Database without a single point of failure by mirroring components.
      The following sections describe the strategies for mirroring the main components of a
      Greenplum system. For a more detailed overview of Greenplum high availability features, see
        <xref
        href="../highavail/topics/g-overview-of-high-availability-in-greenplum-database.xml#topic2"
      />.</p>

    <section id="segment_mirroring" xml:lang="en">
      <title id="iw157552">About Segment Mirroring</title>

      <p>When you deploy your Greenplum Database system, you can configure <i>mirror</i> segments.
        Mirror segments allow database queries to fail over to a backup segment if the primary
        segment becomes unavailable. Mirroring is strongly recommended for production systems and
        required for Pivotal support. </p>
      <p>The secondary (mirror) segment must always reside on a different host than its primary
        segment to protect against a single host failure. Mirror segments can be arranged over the
        remaining hosts in the cluster in configurations designed to maximize availability or
        minimize the performance degradation when hosts or multiple primary segments fail. </p>
      <p>Two standard mirroring configurations are available when you initialize or expand a
        Greenplum system. The default configuration, called <i>group mirroring</i>, places all the
        mirrors for a host's primary segments on one other host in the cluster. The other standard
        configuration, <i>spread mirroring</i>, can be selected with a command-line option. Spread
        mirroring spreads each host's mirrors over the remaining hosts and requires that there are
        more hosts in the cluster than primary segments per host. </p>
      <p>
        <xref format="dita" href="#about_ha/iw157574" type="fig"/> shows how table data is
        distributed across segments when spread mirroring is configured.</p>

      <fig id="iw157574">
        <title>Spread Mirroring in Greenplum Database</title>

        <image href="../../graphics/spread-mirroring.png" placement="break"/>
      </fig>

    </section>

    <section id="segment_failover" xml:lang="en">
      <title>Segment Failover and Recovery</title>
      <p>When mirroring is enabled in a Greenplum Database system, the system will automatically
        fail over to the mirror segment if a primary copy becomes unavailable. A Greenplum Database
        system can remain operational if a segment instance or host goes down as long as all the
        data is available on the remaining active segments.</p>

      <p>If the master cannot connect to a segment instance, it marks that segment instance as down
        in the Greenplum Database system catalog and brings up the mirror segment in its place. A
        failed segment instance will remain out of operation until an administrator takes steps to
        bring that segment back online. An administrator can recover a failed segment while the
        system is up and running. The recovery process copies over only the changes that were missed
        while the segment was out of operation.</p>

      <p>If you do not have mirroring enabled, the system will automatically shut down if a segment
        instance becomes invalid. You must recover all failed segments before operations can
        continue.</p>
    </section>

    <section id="master_mirroring" xml:lang="en">
      <title id="iw157589">About Master Mirroring</title>
      <p>You can also optionally deploy a <i>backup</i> or <i>mirror</i> of the master instance on a
        separate host from the master node. A backup master host serves as a <i>warm standby</i> in
        the event that the primary master host becomes unoperational. The standby master is kept up
        to date by a transaction log replication process, which runs on the standby master host and
        synchronizes the data between the primary and standby master hosts.</p>

      <p>If the primary master fails, the log replication process stops, and the standby master can
        be activated in its place. Upon activation of the standby master, the replicated logs are
        used to reconstruct the state of the master host at the time of the last successfully
        committed transaction. The activated standby master effectively becomes the Greenplum
        Database master, accepting client connections on the master port (which must be set to the
        same port number on the master host and the backup master host).</p>

      <p>Since the master does not contain any user data, only the system catalog tables need to be
        synchronized between the primary and backup copies. When these tables are updated, changes
        are automatically copied over to the standby master to ensure synchronization with the
        primary master.</p>

      <fig id="iw157606">
        <title>Master Mirroring in Greenplum Database</title>

        <image height="165px" href="../../graphics/standby_master.jpg" placement="break"
          width="271px"/>
      </fig>
    </section>

    <section id="interconnect_redundancy" xml:lang="en">
      <title id="iw157609">About Interconnect Redundancy</title>

      <p>The <i>interconnect</i> refers to the inter-process communication between the segments and
        the network infrastructure on which this communication relies. You can achieve a highly
        available interconnect using by deploying dual 10-Gigabit Ethernet switches on your network
        and redundant 10-Gigabit connections to the Greenplum Database host (master and segment)
        servers. </p>
    </section>
  </body>
</topic>
