<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Composite//EN"
"ditabase.dtd">
<topic id="topic2" xml:lang="en">
    <title id="iw157419">About the Greenplum Architecture</title>
    <shortdesc>This section provides a high-level description of Greenplum Database
        architecture.</shortdesc>
    <body>
        <section id="about-gpdb">Greenplum Database is a massively parallel processing (MPP)
            database server based on PostgreSQL open-source technology. MPP (also known as a
                <i>shared nothing</i> architecture) refers to systems with two or more processors
            that cooperate to carry out an operation, each processor with its own memory, operating
            system and disks. Greenplum uses this high-performance system architecture to distribute
            the processing load for multi-terabyte data warehouses, and can use all of the system's
            resources in parallel to process a query.<p>Greenplum Database is essentially several
                PostgreSQL database instances acting together as one cohesive database management
                system (DBMS). It is based on PostgreSQL 8.2.15, and in most cases is very similar
                to PostgreSQL with regard to SQL support, features, configuration options, and
                end-user functionality. Database users interact with Greenplum Database as they
                would a regular PostgreSQL DBMS. </p><p>The internals of PostgreSQL have been
                modified or supplemented to support the parallel structure of Greenplum Database.
                For example, the system catalog, query planner, optimizer, query executor, and
                transaction manager components have been modified and enhanced to be able to execute
                queries simultaneously across all of the parallel PostgreSQL database instances. The
                Greenplum <i>interconnect</i> (the networking layer) enables communication between
                the distinct PostgreSQL instances and allows the system to behave as one logical
                database.</p><p>Greenplum Database also includes features designed to optimize
                PostgreSQL for business intelligence (BI) workloads. For example, Greenplum has
                added parallel data loading (external tables), resource management, query
                optimizations, and storage enhancements, which are not found in standard PostgreSQL.
                Many features and optimizations developed by Greenplum make their way into the
                PostgreSQL community. For example, table partitioning is a feature first developed
                by Greenplum, and it is now in standard PostgreSQL.</p><p>Greenplum Database stores
                and processes large amounts of data by distributing the data and processing workload
                across several servers or <i>hosts</i>. Greenplum Database is an <i>array</i> of
                individual databases based upon PostgreSQL 8.2 working together to present a single
                database image. The <i>master</i> is the entry point to the Greenplum Database
                system. It is the database instance to which clients connect and submit SQL
                statements. The master coordinates its work with the other database instances in the
                system, called <i>segments</i>, which store and process the data.</p></section>
        <fig id="iw157440">
            <title>High-Level Greenplum Database Architecture</title>
            <image height="316px" href="../../graphics/highlevel_arch.jpg" placement="break"
                width="397px"/>
        </fig>
        <p>The following topics describe the components that make up a Greenplum Database system and
            how they work together:</p>
        <ul>
            <li id="iw157448">
                <xref format="dita" href="#topic3" type="topic"/>
            </li>
            <li id="iw157452">
                <xref format="dita" href="#topic4" type="topic"/>
            </li>
            <li id="iw157456">
                <xref format="dita" href="#topic5" type="topic"/>
            </li>
            <li id="iw157460">
                <xref format="dita" href="#topic6" type="topic"/>
            </li>
            <li id="iw157464">
                <xref format="dita" href="#topic11" type="topic"/>
            </li>
            <li id="iw157468">
                <xref format="dita" href="#topic12" type="topic"/>
            </li>
        </ul>
    </body>
    <topic id="topic3" xml:lang="en">
        <title id="iw157474">About the Greenplum Master</title>
        <body>
            <p>The <i>master</i> is the entry point to the Greenplum Database system. It is the
                database process that accepts client connections and processes SQL commands that
                system users issue.</p>
            <p>Greenplum Database end-users interact with Greenplum Database (through the master) as
                they would with a typical PostgreSQL database. They connect to the database using
                client programs such as <codeph>psql</codeph> or application programming interfaces
                (APIs) such as JDBC or ODBC.</p>
            <p>The master is where the <i>global system catalog</i> resides. The global system
                catalog is the set of system tables that contain metadata about the Greenplum
                Database system itself. The master does not contain any user data; data resides only
                on the <i>segments</i>. The master authenticates client connections, processes
                incoming SQL commands, distributes workload among segments, coordinates the results
                returned by each segment, and presents the final results to the client program.</p>
        </body>
    </topic>
    <topic id="topic4" xml:lang="en">
        <title id="iw157496">About the Greenplum Segments</title>
        <body>
            <p>In Greenplum Database, the <i>segments</i> are where data is stored and the majority
                of query processing takes place. When a user connects to the database and issues a
                query, processes are created on each segment to handle the work of that query. For
                more information about query processes, see <xref
                    href="../query/parallel-proc.xml#topic1"/>.</p>
            <p>User-defined tables and their indexes are distributed across the available segments
                in a Greenplum Database system; each segment contains a distinct portion of data.
                The database server processes that serve segment data run under the corresponding
                segment instances. Users interact with segments in a Greenplum Database system
                through the master.</p>
            <p>In the recommended Greenplum Database hardware configuration, there is one active
                segment per effective CPU or CPU core. For example, if your segment hosts have two
                dual-core processors, you would have four primary segments per host.</p>
        </body>
    </topic>
    <topic id="topic5" xml:lang="en">
        <title id="iw157520">About the Greenplum Interconnect</title>
        <body>
            <p>The <i>interconnect</i> is the networking layer of Greenplum Database. The
                    <i>interconnect</i> refers to the inter-process communication between segments
                and the network infrastructure on which this communication relies. The Greenplum
                interconnect uses a standard Gigabit Ethernet switching fabric.</p>
            <p>By default, the interconnect uses User Datagram Protocol (UDP) to send messages over
                the network. The Greenplum software performs packet verification beyond what is
                provided by UDP. This means the reliability is equivalent to Transmission Control
                Protocol (TCP), and the performance and scalability exceeds TCP. If the interconnect
                used TCP, Greenplum Database would have a scalability limit of 1000 segment
                instances. With UDP as the current default protocol for the interconnect, this limit
                is not applicable.</p>
        </body>
    </topic>
    <topic id="topic6" xml:lang="en">
        <title id="iw157531">About Redundancy and Failover in Greenplum Database</title>
        <body>
            <p>You can deploy Greenplum Database without a single point of failure. This topic
                explains the redundancy components of Greenplum Database.</p>
            <ul>
                <li id="iw157542">
                    <xref format="dita" href="#topic7" type="topic"/>
                </li>
                <li id="iw157546">
                    <xref format="dita" href="#topic9" type="topic"/>
                </li>
                <li id="iw157550">
                    <xref format="dita" href="#topic10" type="topic"/>
                </li>
            </ul>
        </body>
        <topic id="topic7" xml:lang="en">
            <title id="iw157552">About Segment Mirroring</title>
            <body>
                <p>When you deploy your Greenplum Database system, you can optionally configure
                        <i>mirror</i> segments. Mirror segments allow database queries to fail over
                    to a backup segment if the primary segment becomes unavailable. To configure
                    mirroring, you must have enough hosts in your Greenplum Database system so the
                    secondary (mirror) segment always resides on a different host than its primary
                    segment. <xref format="dita" href="#topic7/iw157574" type="fig"/> shows how
                    table data is distributed across segments when mirroring is configured.</p>
                <fig id="iw157574">
                    <title>Data Mirroring in Greenplum Database</title>
                    <image height="151px" href="../../graphics/mirrorsegs.png" placement="break"
                        width="447px"/>
                </fig>
            </body>
        </topic>
        <topic id="topic8" xml:lang="en">
            <title>Segment Failover and Recovery</title>
            <body>
                <p>When mirroring is enabled in a Greenplum Database system, the system will
                    automatically fail over to the mirror copy if a primary copy becomes
                    unavailable. A Greenplum Database system can remain operational if a segment
                    instance or host goes down as long as all the data is available on the remaining
                    active segments.</p>
                <p>If the master cannot connect to a segment instance, it marks that segment
                    instance as down in the Greenplum Database system catalog and brings up the
                    mirror segment in its place. A failed segment instance will remain out of
                    operation until an administrator takes steps to bring that segment back online.
                    An administrator can recover a failed segment while the system is up and
                    running. The recovery process copies over only the changes that were missed
                    while the segment was out of operation.</p>
                <p>If you do not have mirroring enabled, the system will automatically shut down if
                    a segment instance becomes invalid. You must recover all failed segments before
                    operations can continue.</p>
            </body>
        </topic>
        <topic id="topic9" xml:lang="en">
            <title id="iw157589">About Master Mirroring</title>
            <body>
                <p>You can also optionally deploy a <i>backup</i> or <i>mirror</i> of the master
                    instance on a separate host from the master node. A backup master host serves as
                    a <i>warm standby</i> in the event that the primary master host becomes
                    unoperational. The standby master is kept up to date by a transaction log
                    replication process, which runs on the standby master host and synchronizes the
                    data between the primary and standby master hosts.</p>
                <p>If the primary master fails, the log replication process stops, and the standby
                    master can be activated in its place. Upon activation of the standby master, the
                    replicated logs are used to reconstruct the state of the master host at the time
                    of the last successfully committed transaction. The activated standby master
                    effectively becomes the Greenplum Database master, accepting client connections
                    on the master port (which must be set to the same port number on the master host
                    and the backup master host).</p>
                <p>Since the master does not contain any user data, only the system catalog tables
                    need to be synchronized between the primary and backup copies. When these tables
                    are updated, changes are automatically copied over to the standby master to
                    ensure synchronization with the primary master.</p>
                <fig id="iw157606">
                    <title>Master Mirroring in Greenplum Database</title>
                    <image height="165px" href="../../graphics/standby_master.jpg" placement="break"
                        width="271px"/>
                </fig>
            </body>
        </topic>
        <topic id="topic10" xml:lang="en">
            <title id="iw157609">About Interconnect Redundancy</title>
            <body>
                <p>The <i>interconnect</i> refers to the inter-process communication between the
                    segments and the network infrastructure on which this communication relies. You
                    can achieve a highly available interconnect by deploying dual Gigabit Ethernet
                    switches on your network and redundant Gigabit connections to the Greenplum
                    Database host (master and segment) servers.</p>
            </body>
        </topic>
    </topic>
    <topic id="topic11" xml:lang="en">
        <title id="iw157616">About Parallel Data Loading</title>
        <body>
            <p>In a large scale, multi-terabyte data warehouse, large amounts of data must be loaded
                within a relatively small maintenance window. Greenplum supports fast, parallel data
                loading with its external tables feature. Administrators can also load external
                tables in single row error isolation mode to filter bad rows into a separate error
                table while continuing to load properly formatted rows. Administrators can specify
                an error threshold for a load operation to control how many improperly formatted
                rows cause Greenplum to abort the load operation.</p>
            <p>By using external tables in conjunction with Greenplum Database's parallel file
                server (<codeph>gpfdist</codeph>), administrators can achieve maximum parallelism
                and load bandwidth from their Greenplum Database system.</p>
            <fig id="iw157638">
                <title>External Tables Using Greenplum Parallel File Server (gpfdist)</title>
                <image height="275px" href="../../graphics/ext_tables.jpg" placement="break"
                    width="454px"/>
            </fig>
        </body>
    </topic>
    <topic id="topic12" xml:lang="en">
        <title id="iw157641">About Management and Monitoring</title>
        <body>
            <p>Administrators manage a Greenplum Database system using command-line utilities
                located in <codeph>$GPHOME/bin</codeph>. Greenplum provides utilities for the
                following administration tasks:</p>
            <ul>
                <li id="iw157649">Installing Greenplum Database on an Array</li>
                <li id="iw157650">Initializing a Greenplum Database System</li>
                <li id="iw157651">Starting and Stopping Greenplum Database</li>
                <li id="iw157652">Adding or Removing a Host</li>
                <li id="iw157653">Expanding the Array and Redistributing Tables among New
                    Segments</li>
                <li id="iw157654">Managing Recovery for Failed Segment Instances</li>
                <li id="iw157655">Managing Failover and Recovery for a Failed Master Instance</li>
                <li id="iw157656">Backing Up and Restoring a Database (in Parallel)</li>
                <li id="iw157657">Loading Data in Parallel</li>
                <li id="iw157658">System State Reporting</li>
            </ul>
            <p>Greenplum provides an optional system monitoring and management tool that
                administrators can install and enable with Greenplum Database. Greenplum Command
                Center uses data collection agents on each segment host to collect and store
                Greenplum system metrics in a dedicated database. Segment data collection agents
                send their data to the Greenplum master at regular intervals (typically every 15
                seconds). Users can query the Command Center database to see query and system
                metrics. Greenplum Command Center has a graphical web-based user interface for
                viewing system metrics, which administrators can install separately from Greenplum
                Database. For more information, see the Greenplum Command Center documentation.</p>
            <fig id="iw157682">
                <title>Greenplum Command Center Architecture</title>
                <image height="304px" href="../../graphics/cc_arch_gpdb.png" placement="break"
                    width="299px"/>
            </fig>
        </body>
    </topic>
</topic>
