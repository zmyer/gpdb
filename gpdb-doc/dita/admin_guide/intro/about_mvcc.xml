<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_kl4_pl2_2s">
  <title>About Concurrency Control in Greenplum Database</title>
  <shortdesc>Greenplum Database uses the PostgreSQL Multiversion Concurrency Control (MVCC) model to
    manage concurrent transactions for heap tables. </shortdesc>
  <body>
    <p>Concurrency control in a database management system allows concurrent queries to complete
      with correct results while ensuring the integrity of the database. Traditional databases use a
      two-phase locking protocol that prevents a transaction from modifying data that has been read
      by another concurrent transaction and prevents any concurrent transaction from reading or
      writing data that another transaction has updated. The locks required to coordinate
      transactions add contention to the database, reducing overall transaction throughput. </p>
    <p>Greenplum Database uses the PostgreSQL Multiversion Concurrency Control (MVCC) model to
      manage concurrency for heap tables. With MVCC, each query operates on a snapshot of the
      database when the query starts. While it executes, a query cannot see changes made by other
      concurrent transactions. This ensures that a query sees a consistent view of the database.
      Queries that read rows can never block waiting for transactions that write rows. Conversely,
      queries that write rows cannot be blocked by transactions that read rows. This allows much
      greater concurrency than traditional database systems that employ locks to coordinate access
      between transactions that read and write data.</p>
    <note>Append-optimized tables are managed with a different concurrency control model than the
      MVCC model discussed in this topic. They are intended for "write-once, read-many" applications
      that never, or only very rarely, perform row-level updates.</note>
    <section>
      <title>Snapshots</title>
      <p>The MVCC model depends on the system's ability to manage multiple versions of data rows. A
        query operates on a snapshot of the database at the start of the query. A snapshot is the
        set of rows that are visible at the beginning of a statement or transaction. The snapshot
        ensures the query has a consistent and valid view of the database for the duration of its
        execution. </p>
      <p>Each transaction is assigned a unique <i>transaction ID</i> (XID), an incrementing 32-bit
        value. When a new transaction starts, it is assigned the next XID. A SQL statement that is
        not enclosed in a transaction is treated as a single-statement transaction—the
          <codeph>BEGIN</codeph> and <codeph>COMMIT</codeph> are added implicitly. This is similar
        to autocommit in some database systems. </p>
      <p>When a transaction inserts a row, the XID is saved with the row in the
          <codeph>xmin</codeph> system column. When a transaction deletes a row, the XID is saved in
        the <codeph>xmax</codeph> system column. Updating a row is treated as a delete and an
        insert, so the XID is saved to the <codeph>xmax</codeph> of the current row and the
          <codeph>xmin</codeph> of the newly inserted row. The <codeph>xmin</codeph> and
          <codeph>xmax</codeph> columns, together with the transaction completion status, specify a
        range of transactions for which the version of the row is visible. A transaction can see the
        effects of all transactions less than <codeph>xmin</codeph>, which are guaranteed to be
        committed, but it cannot see the effects of any transaction greater than or equal to
          <codeph>xmax</codeph>. </p>
      <p>Multi-statement transactions must also record which command within a transaction inserted a
        row (<codeph>cmin</codeph>) or deleted a row (<codeph>cmax</codeph>) so that the transaction
        can see changes made by previous commands in the transaction. The command sequence is only
        relevant during the transaction, so the sequence is reset to 0 at the beginning of a
        transaction. </p>
      <p>XID is a property of the database. Each segment database has its own XID sequence that
        cannot be compared to the XIDs of other segment databases. The master coordinates
        distributed transactions with the segments using a cluster-wide <i>session ID number</i>,
        called <codeph>gp_session_id</codeph>. The segments maintain a mapping of distributed
        transaction IDs with their local XIDs. The master coordinates distributed transactions
        across all of the segment with the two-phase commit protocol. If a transaction fails on any
        one segment, it is rolled back on all segments. </p>
      <p>You can see the <codeph>xmin</codeph>, <codeph>xmax</codeph>, <codeph>cmin</codeph>, and
          <codeph>cmax</codeph> columns for any row with a <codeph>SELECT</codeph>
        statement:<codeblock>SELECT xmin, xmax, cmin, cmax, * FROM <varname>tablename</varname>;</codeblock></p>
      <p>Because you run the <codeph>SELECT</codeph> command on the master, the XIDs are the
        distributed transactions IDs. If you could execute the command in an individual segment
        database, the <codeph>xmin</codeph> and <codeph>xmax</codeph> values would be the segment's
        local XIDs.</p>
    </section>
    <section>
      <title>Transaction ID Wraparound</title>
      <p>The MVCC model uses transaction IDs (XIDs) to determine which rows are visible at the
        beginning of a query or transaction. The XID is a 32-bit value, so a database could
        theoretically execute over four billion transactions before the value overflows and wraps to
        zero. However, Greenplum Database uses <i>modulo 2<sup>32</sup></i> arithmetic with XIDs,
        which allows the transaction IDs to wrap around, much as a clock wraps at twelve o'clock.
        For any given XID, there could be about two billion past XIDs and two billion future XIDs.
        This works until a version of a row persists through about two billion transactions, when it
        suddenly appears to be a new row. To prevent this, Greenplum has a special XID, called
          <codeph>FrozenXID</codeph>, which is always considered older than any regular XID it is
        compared with. The <codeph>xmin</codeph> of a row must be replaced with
          <codeph>FrozenXID</codeph> within two billion transactions, and this is one of the
        functions the <codeph>VACUUM</codeph> command performs. </p>
      <p>Vacuuming the database at least every two billion transactions prevents XID wraparound.
        Greenplum Database monitors the transaction ID and warns if a <codeph>VACUUM</codeph>
        operation is required. </p>
      <p>A warning is issued when a significant portion of the transaction IDs are no longer
        available and before transaction ID wraparound
        occurs:<codeblock>WARNING: database "<varname>database_name</varname>" must be vacuumed within <varname>number_of_transactions</varname> transactions</codeblock></p>
      <p>When the warning is issued, a <codeph>VACUUM</codeph> operation is required. If a
          <codeph>VACUUM</codeph> operation is not performed, Greenplum Database stops creating
        transactions to avoid possible data loss when it reaches a limit prior to when transaction
        ID wraparound occurs and issues this error:</p>
      <codeblock>FATAL: database is not accepting commands to avoid wraparound data loss in database "<varname>database_name</varname>"</codeblock>
      <p>See <xref href="../managing/maintain.xml#topic3/np160654"/> for the procedure to recover
        from this error.</p>
      <p>The server configuration parameters <codeph>xid_warn_limit</codeph> and
          <codeph>xid_stop_limit</codeph> control when the warning and error are displayed. The
          <codeph>xid_warn_limit</codeph> parameter specifies the number of transaction IDs before
        the <codeph>xid_stop_limit</codeph> when the warning is issued. The
          <codeph>xid_stop_limit</codeph> parameter specifies the number of transaction IDs before
        wraparound would occur when the error is issued and new transactions cannot be created.</p>
    </section>
    <section id="section_f4m_n5n_fs">
      <title>Transaction Isolation Modes</title>
      <p>The SQL standard describes three phenomena that can occur when database transactions run
          concurrently:<ul id="ul_up2_tq3_pt">
          <li><i>Dirty read</i> – a transaction can read uncommitted data from another concurrent
            transaction. </li>
          <li><i>Non-repeatable read</i> – a row read twice in a transaction can change because
            another concurrent transaction committed changes after the transaction began. </li>
          <li><i>Phantom read</i> – a query executed twice in the same transaction can return two
            different sets of rows because another concurrent transaction added rows.</li>
        </ul></p>
      <p>The SQL standard defines four transaction isolation modes that database systems must
        support: </p>
      <table frame="all" id="table_gxb_qsz_nt">
        <title>Transaction Isolation Modes</title>
        <tgroup cols="4">
          <colspec colname="c1" colnum="1" colwidth="1*"/>
          <colspec colname="c2" colnum="2" colwidth="1*"/>
          <colspec colname="c3" colnum="3" colwidth="1*"/>
          <colspec colname="c4" colnum="4" colwidth="1*"/>
          <thead>
            <row>
              <entry>Level</entry>
              <entry>Dirty Read</entry>
              <entry>Non-Repeatable</entry>
              <entry>Phantom Read</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>Read Uncommitted</entry>
              <entry>Possible</entry>
              <entry>Possible</entry>
              <entry>Possible</entry>
            </row>
            <row>
              <entry>Read Committed</entry>
              <entry>Impossible</entry>
              <entry>Possible</entry>
              <entry>Possible</entry>
            </row>
            <row>
              <entry>Repeatable Read</entry>
              <entry>Impossible</entry>
              <entry>Impossible</entry>
              <entry>Possible</entry>
            </row>
            <row>
              <entry>Serializable</entry>
              <entry>Impossible</entry>
              <entry>Impossible</entry>
              <entry>Impossible</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <p>The Greenplum Database SQL commands allow you to request <codeph>READ UNCOMMITTED</codeph>,
          <codeph>READ COMITTED</codeph>, or <codeph>SERIALIZABLE</codeph>. Greenplum Database
        treats <codeph>READ UNCOMMITTED</codeph> the same as <codeph>READ COMMITTED</codeph>.
        Requesting <codeph>REPEATABLE READ</codeph> produces an error; use
          <codeph>SERIALIZABLE</codeph> instead. The default isolation mode is <codeph>READ
          COMMITTED</codeph>. </p>
      <p>The difference between <codeph>READ COMMITTED</codeph> and <codeph>SERIALIZABLE</codeph> is
        that in <codeph>READ COMMITTED</codeph> mode, each statement in a transaction sees only rows
        committed before the <i>statement</i> started, while in <codeph>SERIALIZABLE</codeph> mode,
        all statements in a transaction see only rows committed before the <i>transaction</i>
        started. </p>
      <p>The <codeph>READ COMMITTED</codeph> isolation mode permits greater concurrency and better
        performance than the <codeph>SERIALIZABLE</codeph> mode. It allows <i>non-repeatable
          reads</i>, where the values in a row retrieved twice in a transaction can differ because
        another concurrent transaction has committed changes since the transaction began.
          <codeph>READ COMMITTED</codeph> mode also permits <i>phantom reads</i>, where a query
        executed twice in the same transaction can return two different sets of rows.</p>
      <p>The <codeph>SERIALIZABLE</codeph> isolation mode prevents both non-repeatable reads and
        phantom reads, but at the cost of concurrency and performance. Each concurrent transaction
        has a consistent view of the database taken at the beginning of execution. A concurrent
        transaction that attempts to modify data modified by another transaction is rolled back.
        Applications that execute transactions in <codeph>SERIALIZABLE</codeph> mode must be
        prepared to handle transactions that fail due to serialization errors. If
          <codeph>SERIALIZABLE</codeph> isolation mode is not required by the application, it is
        better to use <codeph>READ COMMITTED</codeph> mode. </p>
      <p>The SQL standard specifies that concurrent serializable transactions produce the same
        database state they would produce if executed sequentially. The MVCC snapshot isolation
        model prevents dirty reads, non-repeatable reads, and phantom reads without expensive
        locking, but there are other interactions that can occur between some
          <codeph>SERIALIZABLE</codeph> transactions in Greenplum Database that prevent them from
        being truly serializable. These anomalies can often be attributed to the fact that Greenplum
        Database does not perform <i>predicate locking</i>, which means that a write in one
        transaction can affect the result of a previous read in another concurrent transaction. </p>
      <p>Transactions that run concurrently should be examined to identify interactions that are not
        prevented by disallowing concurrent updates of the same data. Problems identified can be
        prevented by using explicit table locks or by requiring the conflicting transactions to
        update a dummy row introduced to represent the conflict. </p>
      <p>The SQL <codeph>SET TRANSACTION ISOLATION LEVEL</codeph> statement sets the isolation mode
        for the current transaction. The mode must be set before any <codeph>SELECT</codeph>,
          <codeph>INSERT</codeph>, <codeph>DELETE</codeph>, <codeph>UPDATE</codeph>, or
          <codeph>COPY</codeph>
        statements:<codeblock>BEGIN;
SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;
...
COMMIT;
</codeblock></p>
      <p>The isolation mode can also be specified as part of the <codeph>BEGIN</codeph>
        statement:<codeblock>BEGIN TRANSACTION ISOLATION LEVEL SERIALIZABLE;</codeblock></p>
      <p>The default transaction isolation mode can be changed for a session by setting the
          <varname>default_transaction_isolation</varname> configuration property.</p>
    </section>
    <section>
      <title>Removing Dead Rows from Tables</title>
      <p>Updating or deleting a row leaves an expired version of the row in the table. When an
        expired row is no longer referenced by any active transactions, it can be removed and the
        space it occupied can be reused. The <codeph>VACUUM</codeph> command removes expired rows
        from tables. </p>
      <p>When expired rows accumulate in a table, the disk files must be extended to accommodate new
        rows. Performance suffers due to the increased disk I/O required to execute queries. This
        condition is called <i>bloat</i> and it should be managed by regularly vacuuming tables. </p>
      <p>The <codeph>VACUUM</codeph> command (without <codeph>FULL</codeph>) can run concurrently
        with other queries. It removes expired rows from pages, and repacks the remaining rows to
        consolidate the free space. If the amount of remaining free space is significant, it adds
        the page to the table's free space map. When Greenplum Database later needs space for new
        rows, it first consults the table's free space map to find pages with available space. If
        none are found, new pages will be appended to the file. </p>
      <p><codeph>VACUUM</codeph> (without <codeph>FULL</codeph>) does not consolidate pages or
        reduce the size of the table on disk. The space it recovers is only available through the
        free space map. To prevent disk files from growing, it is important to run
          <codeph>VACUUM</codeph> often enough, at least once per day, to ensure that the available
        free space can be found through the free space map. It is also important to run
          <codeph>VACUUM</codeph> after running a transaction that updates or deletes a large number
        of rows. </p>
      <p>The <codeph>VACUUM FULL</codeph> command rewrites the table without expired rows, reducing
        the table to its minimum size. There must be sufficient disk space to create the new table,
        and the table is locked until <codeph>VACUUM FULL</codeph> completes. This is very expensive
        compared to the regular <codeph>VACUUM</codeph> command, and can be avoided or postponed by
        vacuuming regularly. It is best to run <codeph>VACUUM FULL</codeph> during a maintenance
        period. An alternative to <codeph>VACUUM FULL</codeph> is to recreate the table with a
          <codeph>CREATE TABLE AS</codeph> statement and then drop the old table. </p>
      <p>The free space map resides in shared memory and keeps track of free space for all tables
        and indexes. Each table or index uses about 60 bytes of memory and each page with free space
        consumes six bytes. Two system configuration parameters configure the size of the free space
        map:</p>
      <parml>
        <plentry>
          <pt>
            <b>max_fsm_pages</b>
          </pt>
          <pd>Sets the maximum number of disk pages that can be added to the shared free space map.
            Six bytes of shared memory are consumed for each page slot. The default is 200000. This
            parameter must be set to at least 16 times the value of <i>max_fsm_relations</i>.</pd>
        </plentry>
        <plentry>
          <pt>
            <b>max_fsm_relations</b>
          </pt>
          <pd>
            <p>Sets the maximum number of relations that will be tracked in the shared memory free
              space map. This parameter should be set to a value larger than the total number of
                <i>tables + indexes + system tables</i>. The default is 1000. About 60 bytes of
              memory are consumed for each relation per segment instance. It is better to set the
              parameter too high than too low.</p>
          </pd>
        </plentry>
      </parml>
      <p>If the free space map is undersized, some disk pages with available space will not be added
        to the map, and that space cannot be reused until at least the next <codeph>VACUUM</codeph>
        command runs. This causes files to grow. </p>
      <p>You can run <codeph>VACUUM VERBOSE <varname>tablename</varname></codeph> to get a report,
        by segment, of the number of dead rows removed, the number of pages affected, and the number
        of pages with usable free space. </p>
      <p>Query the <codeph>pg_class</codeph> system table to find out how many pages a table is
        using across all segments. Be sure to <codeph>ANALYZE</codeph> the table first to get
        accurate data.
        <codeblock>SELECT relname, relpages, reltuples FROM pg_class WHERE relname='<varname>tablename</varname>';</codeblock></p>
      <p>Another useful tool is the <codeph>gp_bloat_diag</codeph> view in the
          <codeph>gp_toolkit</codeph> schema, which identifies bloat in tables by comparing the
        actual number of pages used by a table to the expected number. See "The gp_toolkit
        Administrative Schema" in the <i>Greenplum Database Reference Guide</i> for more about
          <codeph>gp_bloat_diag</codeph>. </p>
    </section>
  </body>
</topic>
