<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic19">
   <!-- hack for HTML/PDF -->
   <title>Using Hadoop Distributed File System (HDFS) Tables</title>
   <body>
      <p>Greenplum Database leverages the parallel architecture of a Hadoop Distributed File System
         to read and write data files efficiently with the <codeph>gphdfs</codeph> protocol. <ph
            otherprops="op-print">There are three steps to using HDFS:</ph></p>
      <ul otherprops="op-print">
         <li id="du228490">
            <xref href="g-one-time-hdfs-protocol-installation.xml#topic20"/>
         </li>
         <li id="du228491">
            <xref href="g-grant-privileges-for-the-hdfs-protocol.xml#topic21"/>
         </li>
         <li id="du228461">
            <xref href="g-specify-hdfs-data-in-an-external-table-definition.xml#topic22"/>
         </li>
      </ul>
      <p otherprops="op-print">For information about  using Greenplum Database external tables with
         Amazon EMR when Greenplum Database is installed on Amazon Web Services (AWS), also see
            <xref href="g-hdfs-emr-config.xml#amazon-emr"/>.</p>
      <p otherprops="op-print">For HDFS, Greenplum Database supports these additional HDFS-specific
         file formats:<ul id="ul_rvl_tnk_bt">
            <li><xref href="g-hdfs-avro-format.xml#topic_fstrm">Avro File Format</xref></li>
            <li><xref href="g-hdfs-parquet-format.xml#topic_fstrm">Parquet File Format</xref></li>
         </ul></p>
   </body>
</topic>
