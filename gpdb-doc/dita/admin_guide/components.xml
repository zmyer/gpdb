<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Composite//EN" "ditabase.dtd">
<topic id="topic1" xml:lang="en">
    <title id="kf157457">About the Greenplum Architecture</title>
    <body>
        <p>Greenplum Database stores and processes large amounts of data by distributing the data
            and processing workload across several servers or <i>hosts</i>. Greenplum Database is an
                <i>array</i> of individual databases based upon PostgreSQL 8.2 working together to
            present a single database image. The <i>master</i> is the entry point to the Greenplum
            Database system. It is the database instance to which clients connect and submit SQL
            statements. The master coordinates its work with the other database instances in the
            system, called <i>segments</i>, which store and process the data.</p>
        <fig id="kf157478">
            <title>High-Level Greenplum Database Architecture</title>
            <image href="../graphics/highlevel_arch.jpg" placement="break" width="397px"
                height="316px"/>
        </fig>
        <p>This section describes the components that make up a Greenplum Database system and how
            they work together:</p>
        <ul>
            <li id="kf137434">
                <p><xref href="#topic2" type="topic" format="dita"/></p>
            </li>
            <li id="kf137442">
                <p><xref href="#topic3" type="topic" format="dita"/></p>
            </li>
            <li id="kf137447">
                <p><xref href="#topic4" type="topic" format="dita"/></p>
            </li>
            <li id="kf142673">
                <p><xref href="#topic5" type="topic" format="dita"/></p>
            </li>
            <li id="kf142682">
                <p><xref href="#topic10" type="topic" format="dita"/></p>
            </li>
            <li id="kf144713">
                <p><xref href="#topic11" type="topic" format="dita"/></p>
            </li>
        </ul>
    </body>
    <topic id="topic2" xml:lang="en">
        <title id="kf137019">About the Greenplum Master</title>
        <body>
            <p>The <i>master</i> is the entry point to the Greenplum Database system. It is the
                database process that accepts client connections and processes SQL commands that
                system users issue. </p>
            <p>Greenplum Database end-users interact with Greenplum Database (through the master) as
                they would with a typical PostgreSQL database. They connect to the database using
                client programs such as <codeph>psql</codeph> or application programming interfaces
                (APIs) such as JDBC or ODBC.</p>
            <p>The master is where the <i>global system catalog</i> resides. The global system
                catalog is the set of system tables that contain metadata about the Greenplum
                Database system itself. The master does not contain any user data; data resides only
                on the <i>segments</i>. The master authenticates client connections, processes
                incoming SQL commands, distributes workload among segments, coordinates the results
                returned by each segment, and presents the final results to the client program.</p>
        </body>
    </topic>
    <topic id="topic3" xml:lang="en">
        <title id="kf135461">About the Greenplum Segments</title>
        <body>
            <p>In Greenplum Database, the <i>segments</i> are where data is stored and the majority
                of query processing takes place. When a user connects to the database and issues a
                query, processes are created on each segment to handle the work of that query. For
                more information about query processes, see the Greenplum Database
                Administrator Guide. </p>
            <p>User-defined tables and their indexes are distributed across the available segments
                in a Greenplum Database system; each segment contains a distinct portion of data.
                The database server processes that serve segment data run under the corresponding
                segment instances. Users interact with segments in a Greenplum Database system
                through the master. </p>
            <p>In the recommended Greenplum Database hardware configuration, there is one active
                segment per effective CPU or CPU core. For example, if your segment hosts have two
                dual-core processors, you would have four primary segments per host.</p>
        </body>
    </topic>
    <topic id="topic4" xml:lang="en">
        <title id="kf137313">About the Greenplum Interconnect</title>
        <body>
            <p>The <i>interconnect</i> is the networking layer of Greenplum Database. The
                    <i>interconnect</i> refers to the inter-process communication between segments
                and the network infrastructure on which this communication relies. The Greenplum
                interconnect uses a standard Gigabit Ethernet switching fabric.</p>
            <p>By default, the interconnect uses User Datagram Protocol (UDP) to send messages over
                the network. The Greenplum software performs packet verification beyond what is
                provided by UDP. This means the reliability is equivalent to Transmission Control
                Protocol (TCP), and the performance and scalability exceeds TCP. If the interconnect
                used TCP, Greenplum Database would have a scalability limit of 1000 segment
                instances. With UDP as the current default protocol for the interconnect, this limit
                is not applicable.</p>
        </body>
    </topic>
    <topic id="topic5" xml:lang="en">
        <title id="kf153795">About Redundancy and Failover in Greenplum Database</title>
        <body>
            <p>You can deploy Greenplum Database without a single point of failure. This section
                explains the redundancy components of Greenplum Database.</p>
            <ul>
                <li id="kf142817">
                    <p><xref href="#topic6" type="topic" format="dita"/></p>
                </li>
                <li id="kf142830">
                    <p><xref href="#topic8" type="topic" format="dita"/></p>
                </li>
                <li id="kf143054">
                    <p><xref href="#topic9" type="topic" format="dita"/></p>
                </li>
            </ul>
        </body>
        <topic id="topic6" xml:lang="en">
            <title id="kf138923">About Segment Mirroring</title>
            <body>
                <p>When you deploy your Greenplum Database system, you can optionally configure
                        <i>mirror</i> segments. Mirror segments allow database queries to fail over
                    to a backup segment if the primary segment becomes unavailable. To configure
                    mirroring, you must have enough hosts in your Greenplum Database system so the
                    secondary (mirror) segment always resides on a different host than its primary
                    segment. <xref href="#topic6/kf138942" type="fig" format="dita"/>shows how table
                    data is distributed across segments when mirroring is configured..</p>
                <fig id="kf138942">
                    <title>Data Mirroring in Greenplum Database</title>
                    <image href="../graphics/mirrorsegs.png" placement="break" width="447px"
                        height="151px"/>
                </fig>
            </body>
            <topic id="topic7" xml:lang="en">
                <title>Segment Failover and Recovery</title>
                <body>
                    <p>When mirroring is enabled in a Greenplum Database system, the system will
                        automatically fail over to the mirror copy if a primary copy becomes
                        unavailable. A Greenplum Database system can remain operational if a segment
                        instance or host goes down as long as all the data is available on the
                        remaining active segments. </p>
                    <p>If the master cannot connect to a segment instance, it marks that segment
                        instance as down in the Greenplum Database system catalog and brings up the
                        mirror segment in its place. A failed segment instance will remain out of
                        operation until an administrator takes steps to bring that segment back
                        online. An administrator can recover a failed segment while the system is up
                        and running. The recovery process copies over only the changes that were
                        missed while the segment was out of operation.</p>
                    <p>If you do not have mirroring enabled, the system will automatically shut down
                        if a segment instance becomes invalid. You must recover all failed segments
                        before operations can continue.</p>
                </body>
            </topic>
        </topic>
        <topic id="topic8" xml:lang="en">
            <title id="kf137396">About Master Mirroring</title>
            <body>
                <p>You can also optionally deploy a <i>backup</i> or <i>mirror</i> of the master
                    instance on a separate host from the master node. A backup master host serves as
                    a <i>warm standby</i> in the event that the primary master host becomes
                    unoperational. The standby master is kept up to date by a transaction log
                    replication process, which runs on the standby master host and synchronizes the
                    data between the primary and standby master hosts. </p>
                <p>If the primary master fails, the log replication process stops, and the standby
                    master can be activated in its place. Upon activation of the standby master, the
                    replicated logs are used to reconstruct the state of the master host at the time
                    of the last successfully committed transaction. The activated standby master
                    effectively becomes the Greenplum Database master, accepting client connections
                    on the master port (which must be set to the same port number on the master host
                    and the backup master host). </p>
                <p>Since the master does not contain any user data, only the system catalog tables
                    need to be synchronized between the primary and backup copies. When these tables
                    are updated, changes are automatically copied over to the standby master to
                    ensure synchronization with the primary master.</p>
                <fig id="kf142982">
                    <title>Master Mirroring in Greenplum Database</title>
                    <image href="../graphics/standby_master.jpg" placement="break" width="271px"
                        height="165px"/>
                </fig>
            </body>
        </topic>
        <topic id="topic9" xml:lang="en">
            <title id="kf139238">About Interconnect Redundancy</title>
            <body>
                <p>The <i>interconnect</i> refers to the inter-process communication between the
                    segments and the network infrastructure on which this communication relies. You
                    can achieve a highly available interconnect by deploying dual Gigabit Ethernet
                    switches on your network and redundant Gigabit connections to the Greenplum
                    Database host (master and segment) servers.</p>
            </body>
        </topic>
    </topic>
    <topic id="topic10" xml:lang="en">
        <title id="kf137486">About Parallel Data Loading</title>
        <body>
            <p>In a large scale, multi-terabyte data warehouse, large amounts of data must be loaded
                within a relatively small maintenance window. Greenplum supports fast, parallel data
                loading with its external tables feature. Administrators can also load external
                tables in single row error isolation mode to filter bad rows into a separate error
                table while continuing to load properly formatted rows. Administrators can specify
                an error threshold for a load operation to control how many improperly formatted
                rows cause Greenplum to abort the load operation.</p>
            <p>By using external tables in conjunction with Greenplum Database's parallel file
                server (<codeph>gpfdist</codeph>), administrators can achieve maximum parallelism
                and load bandwidth from their Greenplum Database system. </p>
            <fig id="kf141848">
                <title>External Tables Using Greenplum Parallel File Server (gpfdist)</title>
                <image href="../graphics/ext_tables.jpg" placement="break" width="454px"
                    height="275px"/>
            </fig>
        </body>
    </topic>
    <topic id="topic11" xml:lang="en">
        <title id="kf144490">About Management and Monitoring</title>
        <body>
            <p>Administrators manage a Greenplum Database system using command-line utilities
                located in <codeph>$GPHOME/bin</codeph>. Greenplum provides utilities for the
                following administration tasks: </p>
            <ul>
                <li id="kf144833">
                    <p>Installing Greenplum Database on an Array </p>
                </li>
                <li id="kf144834">
                    <p>Initializing a Greenplum Database System </p>
                </li>
                <li id="kf144835">
                    <p>Starting and Stopping Greenplum Database </p>
                </li>
                <li id="kf144836">
                    <p>Adding or Removing a Host </p>
                </li>
                <li id="kf147073">
                    <p>Expanding the Array and Redistributing Tables among New Segments</p>
                </li>
                <li id="kf144837">
                    <p>Managing Recovery for Failed Segment Instances </p>
                </li>
                <li id="kf144838">
                    <p>Managing Failover and Recovery for a Failed Master Instance </p>
                </li>
                <li id="kf144839">
                    <p>Backing Up and Restoring a Database (in Parallel) </p>
                </li>
                <li id="kf144840">
                    <p>Loading Data in Parallel </p>
                </li>
                <li id="kf144985">
                    <p>System State Reporting </p>
                </li>
            </ul>
            <p>Greenplum provides an optional system monitoring and management tool that
                administrators can install and enable with Greenplum Database. Greenplum Command
                Center uses data collection agents on each segment host to collect and store
                Greenplum system metrics in a dedicated database. Segment data collection agents
                send their data to the Greenplum master at regular intervals (typically every 15
                seconds). Users can query the Command Center database to see query and system
                metrics. Greenplum Command Center has a graphical web-based user interface for
                viewing system metrics, which administrators can install separately from Greenplum
                Database. For more information, see the Greenplum Command Center documentation.</p>
            <fig id="kf145043">
                <title>Greenplum Command Center Architecture</title>
                <image href="../graphics/cc_arch_gpdb.png" placement="break" width="299px"
                    height="304px"/>
            </fig>
        </body>
    </topic>
</topic>
